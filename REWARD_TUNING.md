"""
報酬設計のトラブルシューティング手順

v14での根本的な修正
"""

# ✅ v14で実施済みの修正

## 問題: 中間報酬が支配的でライン消去を学習しない
- 平均報酬: 5000-12000点（ほぼ全て中間報酬から）
- 平均ライン消去: 0.0-0.2行
- AIは行を埋めるだけでライン消去を学習しない

## 解決策: 報酬のバランスを根本的に再設計（v14）

### 実施した変更:
1. **ライン消去報酬を大幅増加**: 300 → 1000 (+233%)
2. **中間報酬を大幅削減**: 3.0-80.0 → 0.05-1.0 (-98%以上)
3. **生存・配置報酬を削減**: 生存 1.0→0.1、配置 3.0→0.5
4. **ゲームオーバーペナルティ増加**: 10 → 100

### 結果:
- ライン消去なし: 累積報酬 約-250点/エピソード
- 1ライン消去: +1000点
- 中間報酬は方向性のみ（0.05-1.0点）

---

# ❓ さらなる調整が必要な場合

v14でも学習が不十分な場合、以下を段階的に試してください:

## ステップ1: 学習エピソード数を増やす（最優先）
```bash
python train_gpu.py --episodes 3000  # 1000 → 3000
python train_gpu.py --episodes 5000  # 1000 → 5000
```
v14では報酬が根本的に変わったため、学習に時間がかかります。
まず十分なエピソード数で学習させてください。

## ステップ2: 学習率を調整
```python
# dqn_agent.py で以下を変更:
self.learning_rate = 0.0005  # 0.001 → 0.0005に削減
# または
self.learning_rate = 0.002   # 0.001 → 0.002に増加
```

## ステップ3: 探索期間を調整
```python
# dqn_agent.py で以下を変更:
self.epsilon_decay = 0.997  # 0.995 → 0.997に（探索をより長く）
self.epsilon_min = 0.15     # 0.1 → 0.15に（常に15%の探索）
```

## ステップ4: 中間報酬を微調整（最終手段）
```python
# tetris_env.py で以下を微調整:
ONE_AWAY_FROM_CLEAR_REWARD = 2.0   # 1.0 → 2.0に（少しだけ増加）
VERY_FULL_LINE_REWARD = 1.0        # 0.5 → 1.0に（少しだけ増加）
```
**注意**: 中間報酬を増やしすぎると元の問題に戻ります。
ライン消去報酬（1000-16000）が常に支配的であることを確認してください。

---

# 📊 学習のモニタリング

v14での正常な学習パターン:

### 初期段階（Episode 0-500）:
- 平均報酬: -200 〜 -300（負）
- 平均ライン消去: 0.0 〜 0.1
- **これは正常です**: ランダム探索中

### 学習開始（Episode 500-1500）:
- 平均報酬: -100 〜 +100（徐々に増加）
- 平均ライン消去: 0.1 〜 0.5（徐々に増加）
- **ブレークスルー期**: ライン消去を学習開始

### 安定期（Episode 1500-3000+）:
- 平均報酬: +200 〜 +1000（正の値で安定）
- 平均ライン消去: 0.5 〜 2.0+（安定）
- **成功**: ライン消去を主目標として学習

---

# ⚠️ やってはいけないこと

1. **中間報酬を大幅に増やす**: v13の問題に戻ります
2. **生存報酬を大幅に増やす**: ライン消去よりも長生きを優先してしまいます
3. **ゲームオーバーペナルティを削減**: 死ぬことを恐れなくなります
4. **学習率を大幅に増やす**: 学習が不安定になります

---

# 📝 推奨される対応順序

1. **まず3000-5000エピソード学習させる** ← 最優先
2. 結果を確認（Avg Lines が 0.5以上になったか？）
3. 改善がない場合、ステップ2-3を試す
4. それでも改善がない場合のみ、ステップ4を検討

v14の報酬構造は理論的に正しいため、まずは十分なエピソード数で学習させることが最も重要です。
