# 学習進捗の改善について / Learning Progress Improvements

## 問題 / Issues Fixed

### 1. フリーズの問題 / Freeze Issue
**問題**: 学習済みAIを実行すると、起動から約5秒後にフリーズする
**原因**: `play.py`のメインループでpygameイベントを処理していなかった
**解決**: pygameイベントループを追加し、`pygame.QUIT`イベントを処理

### 2. 学習が進まない問題 / Learning Not Progressing  
**問題**: 1000ステップ経過しても1ラインも消さない
**原因**: 
- 報酬設計が複雑すぎた（時間減衰機構など）
- 報酬値のバランスが悪かった
- 中間報酬が弱すぎた

## 実施した改善 / Improvements Made

### 報酬設計の改善 / Reward Design Improvements

#### v11-v13の変更:
1. **時間減衰の削除**: 中間報酬の時間減衰機構を削除（学習を複雑にしていた）
2. **段階的な報酬**: 50%→70%→80%→90%→90%+の細かい段階で報酬を付与
3. **報酬値の調整**:
   - ライン消去基本報酬: 100 → 200 → 300
   - 1ライン: 300点
   - 2ライン: 1200点
   - 3ライン: 2700点
   - 4ライン: 4800点

4. **中間報酬の強化**:
   - 50%以上満杯: 3点
   - 70%以上満杯: 8点
   - 80%以上満杯: 20点
   - 90%以上満杯: 40点
   - 9/10埋まり: 80点

5. **ペナルティの強化**:
   - 穴のペナルティ: 0.1 → 3.0
   - 高さペナルティ: 0.0 → 0.5
   - 凹凸ペナルティ: 0.0 → 0.3

6. **生存・配置報酬の強化**:
   - 生存報酬: 0.1 → 1.0
   - 配置報酬: 0.5 → 3.0

7. **深度ボーナス**: 画面下部の行を埋めるとより高い報酬（最大1.5倍）

8. **複数行ボーナス**: 2行以上がほぼ満杯の場合、報酬1.5倍

### ハイパーパラメータの改善 / Hyperparameter Improvements

- **学習率**: 0.0001 → 0.001（学習を加速）
- **イプシロン減衰**: 0.9995 → 0.995（探索期間を延長）
- **イプシロン最小値**: 0.05 → 0.1（常に10%の探索を維持）
- **バッチサイズ**: 128 → 64（学習を速く）
- **リプレイバッファ**: 10000 → 50000（多様な経験を保存）
- **勾配クリッピング**: 0.5 → 1.0（緩和）

## 学習の進捗 / Learning Progress

テスト結果（100エピソード）:
- 初期10エピソード平均報酬: 112点
- 最終10エピソード平均報酬: 417点
- **報酬は着実に向上しており、エージェントは行を埋める方法を学習中**

ただし、100エピソードではまだラインクリアには至っていません。
これは正常で、テトリスのラインクリアは長い行動シーケンスが必要なため、
より多くのエピソード（500-2000エピソード）が必要です。

## 推奨される学習設定 / Recommended Training Settings

### 初心者向け / For Beginners
```bash
python train_gpu.py --episodes 500 --max-steps 1000
```
- 500エピソード、各最大1000ステップ
- 所要時間: CPU約30-60分、GPU約10-20分

### 標準的な学習 / Standard Training  
```bash
python train_gpu.py --episodes 2000 --max-steps 1000
```
- 2000エピソード、各最大1000ステップ
- 所要時間: CPU約2-4時間、GPU約30-90分

### 高品質な学習 / High-Quality Training
```bash
python train_gpu.py --episodes 5000 --max-steps 1000
```
- 5000エピソード、各最大1000ステップ
- 所要時間: CPU約5-10時間、GPU約1.5-3時間

## 学習のモニタリング / Monitoring Training

学習中は以下の指標を確認してください:

1. **平均報酬の向上**: 負の値から正の値へ、そしてさらに増加
2. **ライン消去数**: 最初は0だが、徐々に増加していく
3. **エピソード長**: 生存時間が長くなる
4. **Epsilon値**: 1.0から徐々に減少（探索から活用へ）

## トラブルシューティング / Troubleshooting

### 報酬が増えない場合
- 学習率を確認: `dqn_agent.py`の`self.learning_rate`
- エピソード数を増やす: より多くの探索が必要

### メモリ不足の場合
- バッチサイズを減らす: `dqn_agent.py`の`self.batch_size`を32に
- リプレイバッファを減らす: `ReplayBuffer(capacity=20000)`に

### GPUが使えない場合
- CPUで学習可能だが時間がかかる
- `train.py`の代わりに`train_gpu.py`を使用（自動でCPU/GPUを検出）

## 今後の改善案 / Future Improvements

1. **優先度付き経験再生 (Prioritized Experience Replay)**: 重要な経験を優先的に学習
2. **Dueling DQN**: Q値の推定精度を向上
3. **より洗練された報酬設計**: ドメイン知識を活用
4. **カリキュラム学習**: 簡単なタスクから徐々に難しく

## 参照 / References

- DQN原論文: [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)
- テトリスAI: [Tetris AI using Genetic Algorithm](https://codemyroad.wordpress.com/2013/04/14/tetris-ai-the-near-perfect-player/)
