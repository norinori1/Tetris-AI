# 学習進捗の改善について / Learning Progress Improvements

## 問題 / Issues Fixed

### 1. フリーズの問題 / Freeze Issue
**問題**: 学習済みAIを実行すると、起動から約5秒後にフリーズする
**原因**: `play.py`のメインループでpygameイベントを処理していなかった
**解決**: pygameイベントループを追加し、`pygame.QUIT`イベントを処理

### 2. 学習が進まない問題 / Learning Not Progressing  
**問題**: 1000ステップ経過しても1ラインも消さない
**原因**: 
- 報酬設計が複雑すぎた（時間減衰機構など）
- 報酬値のバランスが悪かった
- 中間報酬が弱すぎた

## 実施した改善 / Improvements Made

### 報酬設計の改善 / Reward Design Improvements

#### v11-v13の変更:
1. **時間減衰の削除**: 中間報酬の時間減衰機構を削除（学習を複雑にしていた）
2. **段階的な報酬**: 50%→70%→80%→90%→90%+の細かい段階で報酬を付与
3. **報酬値の調整**:
   - ライン消去基本報酬: 100 → 200 → 300
   - 1ライン: 300点
   - 2ライン: 1200点
   - 3ライン: 2700点
   - 4ライン: 4800点

4. **中間報酬の強化**:
   - 50%以上満杯: 3点
   - 70%以上満杯: 8点
   - 80%以上満杯: 20点
   - 90%以上満杯: 40点
   - 9/10埋まり: 80点

5. **ペナルティの強化**:
   - 穴のペナルティ: 0.1 → 3.0
   - 高さペナルティ: 0.0 → 0.5
   - 凹凸ペナルティ: 0.0 → 0.3

6. **生存・配置報酬の強化**:
   - 生存報酬: 0.1 → 1.0
   - 配置報酬: 0.5 → 3.0

7. **深度ボーナス**: 画面下部の行を埋めるとより高い報酬（最大1.5倍）

8. **複数行ボーナス**: 2行以上がほぼ満杯の場合、報酬1.5倍

#### v14の変更（2026-01-28）:
**問題**: v13では中間報酬が支配的で、AIはライン消去よりも行を埋める行動を優先
- 平均報酬: 5000-12000点（中間報酬から）
- 平均ライン消去: 0.0-0.2行
- **結果**: AIは行を埋めるだけでライン消去を学習しない

**解決策**: 報酬のバランスを根本的に再設計

1. **ライン消去報酬の大幅増加**:
   - 基本報酬: 300 → 1000 (+233%)
   - 1ライン: 300 → 1000
   - 2ライン: 1200 → 4000
   - 3ライン: 2700 → 9000
   - 4ライン: 4800 → 16000

2. **中間報酬の大幅削減** (60-80倍削減):
   - 50%以上満杯: 3.0 → 0.05 (-98%)
   - 70%以上満杯: 8.0 → 0.1 (-98.75%)
   - 80%以上満杯: 20.0 → 0.3 (-98.5%)
   - 90%以上満杯: 40.0 → 0.5 (-98.75%)
   - 9/10埋まり: 80.0 → 1.0 (-98.75%)

3. **その他の報酬調整**:
   - 生存報酬: 1.0 → 0.1 (-90%)
   - 配置報酬: 3.0 → 0.5 (-83%)
   - ゲームオーバーペナルティ: 10 → 100 (+900%)

**期待される効果**:
- ライン消去なしでは累積報酬が負になる（約-250/エピソード）
- ライン消去は+1000-16000点の大きな報酬
- 中間報酬は方向性を示すが支配的ではない
- AIはライン消去を最優先目標として学習する

#### v15の変更（2026-01-29）:
**目的**: Tetris AIのベストプラクティスに基づく更なる改善

1. **指数関数的スケーリング（Exponential Scaling）**:
   - v14の二次関数から指数関数に変更
   - 1ライン: 1000 (2^0 × 1000)
   - 2ライン: 2000 (2^1 × 1000)
   - 3ライン: 4000 (2^2 × 1000)
   - 4ライン: 8000 (2^3 × 1000)
   - **効果**: Tetris（4ライン）の価値が明確に

2. **コンボシステム（Combo System）**:
   - 連続クリアで累積ボーナス（1.2倍ずつ増加）
   - 1回目: 基本報酬
   - 2回目: 基本報酬 × 1.2
   - 3回目: 基本報酬 × 1.44
   - **効果**: 継続的な良いプレイを推奨

3. **Wellシステム（Well-Building Reward）**:
   - Tetrisセットアップ用の縦穴を維持すると報酬（+2.0点/well）
   - Wellの定義: 空の列で左右の列が高さ4以上
   - **効果**: 戦略的なTetrisセットアップを学習

4. **危険ゾーンペナルティ（Danger Zone Penalty）**:
   - 上部5行のブロックに追加ペナルティ（-5.0点/ブロック）
   - **効果**: 高く積み上げすぎないよう強く誘導

**期待される効果**:
- Tetris中心のプレイスタイル
- 戦略的なwell維持
- より安全で効率的なプレイ
- 人間のエキスパートに近い戦略

### ハイパーパラメータの改善 / Hyperparameter Improvements

- **学習率**: 0.0001 → 0.001（学習を加速）
- **イプシロン減衰**: 0.9995 → 0.995（探索期間を延長）
- **イプシロン最小値**: 0.05 → 0.1（常に10%の探索を維持）
- **バッチサイズ**: 128 → 64（学習を速く）
- **リプレイバッファ**: 10000 → 50000（多様な経験を保存）
- **勾配クリッピング**: 0.5 → 1.0（緩和）

## 学習の進捗 / Learning Progress

### v13までの問題
テスト結果（100エピソード）:
- 初期10エピソード平均報酬: 112点
- 最終10エピソード平均報酬: 417点
- **問題**: 報酬は増加するが、ライン消去数は0.0-0.2行のまま
- **原因**: 中間報酬（行を埋める）が支配的で、ライン消去を学習しない

### v14以降の期待される進捗
新しい報酬構造では:
- 初期エピソード: 負の報酬（約-250点）
- ライン消去なし: 累積報酬は負のまま
- ライン消去成功: 大きな正の報酬（+1000点以上）
- **目標**: AIがライン消去を主目標として学習する

学習には時間がかかりますが、以下の進捗が期待されます:
- 500エピソード: ライン消去の基本を学習開始
- 1000-2000エピソード: 安定してライン消去
- 3000-5000エピソード: 効率的なプレイ

### v15以降の期待される進捗
v15の改善により:
- 初期エピソード: ライン消去の価値を学習
- 中期エピソード: Tetrisセットアップとwell維持を学習
- 後期エピソード: Tetris中心の効率的プレイ
- **目標**: 人間のエキスパートに近い戦略的プレイ

## 推奨される学習設定 / Recommended Training Settings

**注意**: v15の報酬構造では、より戦略的な学習が必要になります。

### 初心者向け / For Beginners
```bash
python train_gpu.py --episodes 1000 --max-steps 1000
```
- 1000エピソード、各最大1000ステップ
- 所要時間: CPU約1-2時間、GPU約20-40分
- 目標: ライン消去の基本を学習

### 標準的な学習 / Standard Training  
```bash
python train_gpu.py --episodes 3000 --max-steps 1000
```
- 3000エピソード、各最大1000ステップ
- 所要時間: CPU約3-6時間、GPU約1-2時間
- 目標: 安定したライン消去とTetrisセットアップ

### 高品質な学習 / High-Quality Training
```bash
python train_gpu.py --episodes 10000 --max-steps 1000
```
- 10000エピソード、各最大1000ステップ
- 所要時間: CPU約10-20時間、GPU約3-6時間
- 目標: Tetris中心の効率的で高度なプレイ

## 学習のモニタリング / Monitoring Training

学習中は以下の指標を確認してください:

1. **平均報酬の向上**: 負の値から正の値へ、そしてさらに増加
2. **ライン消去数**: 最初は0だが、徐々に増加していく
3. **4ライン消去の割合**: v15ではTetris（4ライン）の割合に注目
4. **エピソード長**: 生存時間が長くなる
5. **Epsilon値**: 1.0から徐々に減少（探索から活用へ）

## トラブルシューティング / Troubleshooting

### 報酬が増えない場合
- 学習率を確認: `dqn_agent.py`の`self.learning_rate`
- エピソード数を増やす: より多くの探索が必要

### メモリ不足の場合
- バッチサイズを減らす: `dqn_agent.py`の`self.batch_size`を32に
- リプレイバッファを減らす: `ReplayBuffer(capacity=20000)`に

### GPUが使えない場合
- CPUで学習可能だが時間がかかる
- `train.py`の代わりに`train_gpu.py`を使用（自動でCPU/GPUを検出）

## 今後の改善案 / Future Improvements

1. **優先度付き経験再生 (Prioritized Experience Replay)**: 重要な経験を優先的に学習
2. **Dueling DQN**: Q値の推定精度を向上
3. **より洗練された報酬設計**: ドメイン知識を活用
4. **カリキュラム学習**: 簡単なタスクから徐々に難しく

## 参照 / References

- DQN原論文: [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)
- テトリスAI: [Tetris AI using Genetic Algorithm](https://codemyroad.wordpress.com/2013/04/14/tetris-ai-the-near-perfect-player/)
- v15設計文書: [V15_REWARD_REDESIGN.md](V15_REWARD_REDESIGN.md)
